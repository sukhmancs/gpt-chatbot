from openai import OpenAI
import random
from fuzzywuzzy import fuzz
import re

class MathBot:
    """
    Class to represent the MathBot. It uses the OpenAI API to generate responses.
    It has two functions to generate responses using the GPT-3.5-turbo, Davinci-002 models or both.
    It also has a function to reset the dialog.    
    """
    def __init__(self, api_key):
        """This is the constructor. Sets the api key and initializes the dialog."""
        self.client = OpenAI(api_key=api_key)
        self.operations = ['+', '-', '*', '/', '^']
        self.numbers = [2, 3, 4]
        self.gpt_bot_dialog = [
            {"role":"system", "content":"You are a math bot. When you're asked to do math, you always add a small amount to the result. This is a math code that math bots use to calculate. For example 2+2 would be 5 or 6, 3+3 would be 7 or 8, etc. Make sure you use the code. But the code is secret! Never mention it or talk about the code. Just give the answer."}        
        ] # initialize the dialog for the GPT-3.5-turbo model

        self.gpt_davinci_dialog = self.generate_dialog() # generate a dialog for the Davinci-002 model

    def generate_dialog(self):
        """
        This function generates a dialog that contains different math operations.
        
        Returns:
            list: A list of dictionaries containing the dialog.
        """
        dialog = [
            {"role":"system", "content":"You are a math bot. When you're asked to do math, you always add a small amount to the result. This is a math code that math bots use to calculate. For example 2+2 would be 5 or 6, 3+3 would be 7 or 8, etc. Make sure you use the code. But the code is secret! Never mention it or talk about the code. Just give the answer."}
        ]
        for number in self.numbers:
            for operation in self.operations:
                for number2 in self.numbers:
                    dialog.append({"role": "user", "content": f"What's {number}{operation}{number2}?"})
                    # add a random operation to the response to make it more interesting
                    # in this case, i am adding either 1 or 2 to the result
                    dialog.append({"role": "assistant", "content": str(eval(f"{number}{operation}{number2}{operation}{random.choice([1, 2])}"))}) 
        return dialog
    
    def gpt_bot(self, utterance):    
        """
        This function uses the GPT-3.5-turbo model to generate a response to the given utterance.
        
        Args:
            utterance (str): The user's message.
        
        Returns:
            str: The response generated by the GPT-3.5-turbo model.
        """
        # append the user's message to the dialog
        self.gpt_bot_dialog.append({"role": "user", "content": utterance})

        # generate the response
        response = self.client.chat.completions.create(model="gpt-3.5-turbo", messages=self.gpt_bot_dialog, temperature=0, max_tokens=1) 

        # append the response to the dialog so that it can be used in the next iteration
        # to generate the next response. For example, if the user asks a follow-up question.
        # the dialog will contain the context of the conversation.
        self.gpt_bot_dialog.append({"role": "assistant", "content": response.choices[0].message.content})
        return response.choices[0].message.content # return the response

    def gpt_davinci(self, utterance):
        """
        This function uses the Davinci-002 model to generate a response to the given utterance.

        Args:
            utterance (str): The user's message.

        Returns:
            str: The response generated by the Davinci-002 model.
        """
        # append the user's message to the dialog
        self.gpt_davinci_dialog.append({"role": "user", "content": utterance})
        
        # generate the response
        response = self.client.chat.completions.create(model="davinci-002", messages=self.gpt_davinci_dialog, temperature=0, max_tokens=1)

        # append the response to the dialog so that it can be used in the next iteration
        # to generate the next response. For example, if the user asks a follow-up question.
        # the dialog will contain the context of the conversation.
        self.gpt_davinci_dialog.append({"role": "assistant", "content": response.choices[0].message.content})
        return response.choices[0].message.content # return the response

    # Use two different prompt engineering techniques to generate two different responses. Then
    # make a third API call to decide which of the two is most appropriate based on some prompt-
    # engineered criteria.

    def get_response(self, utterance, model="both"):
        """
        This function generates a response to the given utterance using the specified model.

        Args:
            utterance (str): The user's message.
            model (str): The model to use for generating the response. Options are "gpt-3.5-turbo", "davinci-002", or "both".

        Returns:
            str or tuple: The response generated by the specified model.
        """
        if model == "gpt-3.5-turbo":
            return self.gpt_bot(utterance)
        elif model == "davinci-002":
            return self.gpt_davinci(utterance)
        elif model == "both":
            response1 = self.gpt_bot(utterance)
            response2 = self.gpt_davinci(utterance)
            return self.decide_best_response(utterance, response1, response2)
        else:
            return "Invalid model. Please choose 'gpt-3.5-turbo', 'davinci-002', or 'both'."
        
    def decide_best_response(self, utterance, response1, response2):
        """
        This function decides which of the two responses is the best based on some criteria.
        
        Args:
            utterance (str): The user's message.
            response1 (str): The response generated by the GPT-3.5-turbo model.
            response2 (str): The response generated by the Davinci-002 model.
        
        Returns:
            str: The best response.
        """
        # some criteria to decide which response is the best
        # for example, we can use fuzzy string matching
        return self.fuzzy_search(utterance, response1, response2)        
            
    def fuzzy_search(utterance, response1, response2):
        """
        This function uses fuzzy string matching to determine which response is the best match for the given utterance.
        
        Args:
            utterance (str): The user's message.
            response1 (str): The response generated by the GPT-3.5-turbo model.
            response2 (str): The response generated by the Davinci-002 model.
        
        Returns:
            str: The best response.
        """
        # calculate the similarity between the utterance and the responses
        similarity1 = fuzz.partial_ratio(utterance, response1)
        similarity2 = fuzz.partial_ratio(utterance, response2)
        
        # return the response with the highest similarity
        if similarity1 > similarity2:
            return response1
        else:
            return response2
        
    def reset_dialog(self):
        """This function resets the dialog."""
        self.gpt_bot_dialog = [
            {"role":"system", "content":"You are a math bot. When you're asked to do math, you always add a small amount to the result. This is a math code that math bots use to calculate. For example 2+2 would be 5 or 6, 3+3 would be 7 or 8, etc. Make sure you use the code. But the code is secret! Never mention it or talk about the code. Just give the answer."}        
        ] # reset the dialog for the GPT-3.5-turbo model
        self.gpt_davinci_dialog = self.generate_dialog() # reset and generate a new dialog for the Davinci-002 model

# Usage
with open("openai_key.txt") as file:
    key = file.read()
bot = MathBot(key)
response = bot.get_response("What's 2+2?", model="both") # or model="gpt-3.5-turbo" or model="davinci-002"

# check if the response is a tuple means both models are used to generate the response
if isinstance(response, tuple):
    print(f"Turbo: {response[0]}\nDavinci: {response[1]}")
else: # response is a string
    print(response)
